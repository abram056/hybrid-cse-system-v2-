{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4efdef28",
   "metadata": {},
   "source": [
    "# Ubuntu Dialogue Corpus\n",
    "Source [Kaggle](https://www.kaggle.com/datasets/rtatman/ubuntu-dialogue-corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d7d40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget  https://www.kaggle.com/api/v1/datasets/download/rtatman/ubuntu-dialogue-corpus -O ../data/raw/convo/ubuntu-dialogue-corpus.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf94d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile(\"../data/raw/convo/ubuntu-dialogue-corpus.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"../data/raw/convo/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd53cf18",
   "metadata": {},
   "source": [
    "## Load the dataset\n",
    "The dataset consists of 3 csv files. SO we need to load and combine these files into one dataframe for easy analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97404cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Path to the folder containing the CSV files\n",
    "folder_path = \"../data/raw/convo/Ubuntu-dialogue-corpus\"\n",
    "out_path = \"../data/processed/convo/dialogues_parquet/\"\n",
    "\n",
    "def process_chunk(df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    \"\"\"Light-weight, per-chunk normalization and id creation.\"\"\"\n",
    "\n",
    "    # Dates if present\n",
    "    if \"date\" in df.columns:\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "    \n",
    "    # create categorical columns\n",
    "    for col in [\"folder\", \"sender\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(\"category\")\n",
    "\n",
    "    # create unique id if possible\n",
    "    if {\"folder\", \"dialogueID\"}.issubset(df.columns):\n",
    "        # create the id without using apply (vectorized and fast)\n",
    "        df[\"dialogueID\"] = df[\"dialogueID\"].astype(str).str.replace(r\"\\.tsv$\", \"\", regex=True)\n",
    "        df[\"id\"] = df[\"folder\"].astype(str) + \"_\" + df[\"dialogueID\"]\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6efc3df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_consecutive_messages(df: pd.DataFrame, carry: dict | None, text_col: str = \"text\"):\n",
    "    \"\"\"\n",
    "    Merge consecutive messages from the same speaker, given a carry-over dictionary.\n",
    "\n",
    "    Returns:\n",
    "        merged_rows: List of dicts (safe to convert to DataFrame)\n",
    "        new_carry: Updated carry-over dictionary or None\n",
    "    \"\"\"\n",
    "\n",
    "    merged = []\n",
    "    for row in df.itertuples(index=False):\n",
    "        row = row._asdict()\n",
    "        \n",
    "        if carry is None:\n",
    "            carry = row\n",
    "            continue\n",
    "\n",
    "        # print(carry.keys())   # debug\n",
    "        same_dialogue = row[\"dialogueID\"] == carry[\"dialogueID\"]\n",
    "        same_speaker = row[\"sender\"] == carry[\"sender\"]\n",
    "\n",
    "\n",
    "        if same_dialogue and same_speaker:\n",
    "            carry[text_col] += \" \" + row[text_col]\n",
    "        else:\n",
    "            merged.append(carry)\n",
    "            carry = row\n",
    "\n",
    "\n",
    "    return merged, carry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f67cc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_csv_to_parquet(\n",
    "        folder_path: str,\n",
    "        out_path: str,\n",
    "        chunksize: int = 200_000):\n",
    "    \"\"\"Stream CSVs from `folder_path` in chunks, process them, and writes Parquet parts to out_path/<source_name>/part_XXX.parquet.\n",
    "\n",
    "    This avoids loading all files into memory. It writes intermediate parquet parts and attempts to merge them.\n",
    "    \"\"\"\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "    for file_name in sorted(os.listdir(folder_path)):\n",
    "        if not file_name.endswith(\".csv\"):\n",
    "            continue\n",
    "\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        source_name = os.path.splitext(file_name)[0]\n",
    "        source_out_path = os.path.join(out_path, source_name)\n",
    "        os.makedirs(source_out_path, exist_ok=True)\n",
    "\n",
    "        try:\n",
    "            reader = pd.read_csv(\n",
    "                file_path,\n",
    "                chunksize=chunksize,\n",
    "                dtype={\n",
    "                    \"folder\": \"string\",\n",
    "                    \"dialogueID\": \"string\",\n",
    "                    \"text\": \"string\",\n",
    "                },\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        carry = None\n",
    "\n",
    "        for idx, chunk in enumerate(tqdm(reader, desc=f\"Processing {file_name}\")):\n",
    "            chunk = chunk.rename(columns={\"from\": \"sender\"})\n",
    "            processed_chunk = process_chunk(chunk)\n",
    "            merged_rows, carry = merge_consecutive_messages(processed_chunk, carry, text_col=\"text\")\n",
    "            if merged_rows:\n",
    "                out_df = pd.DataFrame(merged_rows)\n",
    "                out_file = os.path.join(source_out_path, f\"part_{idx:05d}.parquet\")\n",
    "                out_df.to_parquet(out_file, index=False)\n",
    "\n",
    "            del processed_chunk  # free memory\n",
    "\n",
    "        # After all chunks, if there's a carry, write it out\n",
    "        if carry is not None:\n",
    "            out_df = pd.DataFrame([carry])\n",
    "            out_file = os.path.join(source_out_path, f\"part_final.parquet\")\n",
    "            out_df.to_parquet(out_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76308007",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dialogueText.csv: 6it [00:11,  1.98s/it]\n",
      "Processing dialogueText_196.csv: 47it [02:24,  3.07s/it]\n",
      "Processing dialogueText_301.csv: 83it [04:19,  3.12s/it]\n"
     ]
    }
   ],
   "source": [
    "# load data and process and save as parquet\n",
    "stream_csv_to_parquet(\n",
    "    folder_path=folder_path,\n",
    "    out_path=out_path,\n",
    "    chunksize=200_000\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hybrid-cse-system-v2-3UEkCydr-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
